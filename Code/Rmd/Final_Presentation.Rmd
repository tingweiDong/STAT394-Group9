---
title: "Final Presentation"
author: "Group 9"
date: "2022-09-16"
output: 
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(ggplot2)
require(ggcorrplot)
require(ggthemes)
require(gridExtra)
require(pander)
require(xtable)
require(bookdown)
require(psych)
require(MASS)
require(ggord)
require(klaR)
```

```{r read, include=FALSE}
student=read.table("../../Data/CSV/student-mat.csv",sep=";",header=TRUE)
df_num = data.frame(student$age, student$absences, 
                    student$G1, student$G2, student$G3)
student$school <- as.factor(student$school)
student$sex <- as.factor(student$sex)
student$failures <- as.factor(student$failures)
df = data.frame(student$school,student$sex,student$failures)
names(df) = c("School", "Sex", "Failures")
names(df_num) <- c("age", "absences", "G1","G2","G3")
```

## Members {.smaller .columns-2}
![](../../Images/JPG/Photo/Fugo%20Takefusa.jpg){width="100px"} 
Fugo Takefusa\
[takefufugo\@myvuw.ac.nz](mailto:takefufugo@myvuw.ac.nz){.email}\
ORCID: 0000-0001-7373-8389\

![](../../Images/JPG/Photo/Rhys%20Lewis-Woodley.jpg){width="100px"} 
Rhys Lewis-Woodley\
[lewisrhys\@myvuw.ac.nz](mailto:lewisrhys@myvuw.ac.nz){.email}\
ORCID: 0000-0001-5414-0832\

![](../../Images/JPG/Photo/Soumya%20Banerjee.jpg){width="100px"} 
Soumya Banerjee\
[banerjsoum\@myvuw.ac.nz](mailto:banerjsoum@myvuw.ac.nz){.email}\
ORCID: 0000-0001-5678-5095\

![](../../Images/JPG/Photo/Vivian%20Dong.jpg){width="100px"} 
Vivian Dong\
[dongting\@myvuw.ac.nz](mailto:dongting@myvuw.ac.nz){.email}\
ORCID: 0000-0001-8356-8598\

![](../../Images/JPG/Photo/Yi%20Zhang.jpg){width="100px"}
Yi Zhang\
[zhangyi11\@myvuw.ac.nz](mailto:zhangyi11@myvuw.ac.nz){.email}\
ORCID: 0000-0003-2959-5290\


## Student Performance DataSet

-   The data is about student academic grades for math course of two Portuguese secondary public school during the 2005/2006 school year.
-   The dataset have 395 records in total with 5 numerical variables and 28 categorical variables.
    -   The numerical variables include age, number of school absences, 1st Period grade(G1), 2nd period grade(G2) and final period grade(G3, target variable).
    -   Since we have too many categorical variables, for our data analysis, we will only include school, sex and number of past failures.

## First View of Data {.smaller .columns-2}

```{r ct, fig.height=4, fig.cap="Barplot for Categorical Variables", fig.pos="center", echo=FALSE, warning=FALSE, out.width="99%"}
lapply(names(df), function(col) {
  ggplot(df, aes(.data[[col]], ..count..)) + 
    geom_bar(aes(fill = .data[[col]]), position = "dodge")
}) -> lp
grid.arrange(lp[[1]], lp[[2]], lp[[3]], ncol = 3)
```

-   There are more students from Gabriel Pereira school than Mousinho da Silveira school.
-   There are slightly higher number of female students compared to male students.
-   Most of the students never failed the other courses before.

```{r numplt,fig.height=4, fig.cap="Barplot for Numerical Variables", fig.pos="center", echo=FALSE, warning=FALSE, out.width="99%"}
df_num.mean<-colMeans(df_num)

lp2 <- list()

lp2[[1]] <- ggplot(data.frame(df_num$age), aes(x=df_num$age)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$age),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[1], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("Age") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[2]] <- ggplot(data.frame(df_num$absences), aes(x=df_num$absences)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$absences),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[2], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("Absences") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[3]] <- ggplot(data.frame(df_num$G1), aes(x=df_num$G1)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G1),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[3], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G1") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[4]] <- ggplot(data.frame(df_num$G2), aes(x=df_num$G3)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G3),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[4], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G2") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[5]] <- ggplot(data.frame(df_num$G3), aes(x=df_num$G3)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G3),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[5], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G3") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

grid.arrange(lp2[[1]], lp2[[2]], lp2[[3]],lp2[[4]],lp2[[5]],nrow = 2,ncol = 3)
```

-   Red dashed line as mean and a fitted density in purple.
-   Mean: Age 17, Number of absences 6, G1 grade 11, G2 grade 11, G3 grade 10
-   Variance: Age 1.63, Number of absences 64.05, G1 grade 11.02, G2 grade 14.15, G3 grade 20.99

## EDA

bar plot that shows how many number of students have passed or failed 

## Correlation

```{r corplt,fig.height=4, fig.cap="Correlation between five Numerical Variables", fig.pos="center", echo=FALSE, warning=FALSE, out.width="99%"}
ggcorrplot(cor(df_num),
method = "circle",
hc.order = TRUE,
type = "lower")
```

## Leading Questions

- Can PCA reduce the number of dimensions to model the students' final grade?\

- Can we have a discriminant model to classify students whether they pass or fail based on the other features(e.g., first test grade, age, etc)?\



## PCA

We have conducted PCA on 4 numeric variables (G1, G2, Age, Number of Absences) to see if we can perform dimentionality reduction.

<div class="columns-2">

```{r scree, fig.cap="", echo=FALSE, warning=FALSE, out.width="99%"}
PCA <- prcomp(df_num[, -5])
var_explained = PCA$sdev^2 / sum(PCA$sdev^2)

#create scree plot
qplot(c(1:4), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Proportion of Variance Explained") +
  ggtitle("Scree Plot for PCA in %") +
  ylim(0, 1)
```


<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

|          | PC1      | PC2   |      PC3  | PC3|
|-----------------------|--------|--------|--------|--------|
|Standard deviation     | 8.0088 | 4.8287 | 1.4047 | 1.1888 |
|Proportion of Variance | 0.7061 | 0.2567 | 0.0217 | 0.0156 |
|Cumulative Proportion  | 0.7061 | 0.9627 | 0.9844 | 1.0000 |


## Fit Linear Regression

$$
Full\ model:G3_i = \beta_0 + \beta_1G1_i + \beta_2G2_i + \beta_3Age_i + \beta_4Absence_i + \epsilon_i
$$

$$
PC12\ model:G3_i = \beta_0 + \beta_1PC1_i + \beta_2PC2_i + \epsilon_i
$$

$$
PC123\ model:G3_i = \beta_0 + \beta_1PC1_i + \beta_2PC2_i + \beta_3PC3_i + \epsilon_i
$$

## Diagnostics of Full Model

```{r diag1, fig.cap="", echo=FALSE, warning=FALSE, fig.height=5}
PCAdf <- data.frame(df_num$G3, PCA$x)
modPCA12 <- lm(df_num.G3 ~ PC1 + PC2, data=PCAdf)
modPCA123 <- lm(df_num.G3 ~ PC1 + PC2 + PC3, data=PCAdf)
modNormal <- lm(G3 ~ ., data=df_num)

par(mfrow=c(2,2))
plot(modNormal)
```

## Diagnostics of PC12 Model

```{r diag2, fig.cap="", echo=FALSE, warning=FALSE, fig.height=5}
par(mfrow=c(2,2))
plot(modPCA12)
```

## Diagnostics of PC123 Model

```{r diag3, fig.cap="", echo=FALSE, warning=FALSE, fig.height=5}
par(mfrow=c(2,2))
plot(modPCA123)
```

## Model Comparison

Check AIC, BIC and Adjusted $R^2$
 
|  | AIC | BIC | Adjusted $R^2$ |
|------------|---------|---------|------|
| Full Model | 1637.29 | 1661.16 | 0.83 | 
| ModelPC12  | 1689.13 | 1705.05 | 0.80 | 
| ModelPC123 | 1637.31 | 1657.20 | 0.83 | 


- In contrast with what we have seen in the scree plot before, PC123 model is the 'best' model so far.

- We can reduce the number of variables to 3 instead of using 4 variables to predict the final grade.

## LDA {.smaller}

-   Fail:   G3 grade under $10$. Total Fail students: $130$.\
-   Pass:   G3 grade equal and above $10$. Total Pass students:$265$.\

```{r ldaPairs, echo=FALSE}
G3 <- df_num$G3
Label = c()
for(i in 1:length(G3)){
  if(G3[i]>=10){
    Label[i] <- "Pass"
  }else {
    Label[i] <- "Fail"
  }
}
Label <- data.frame(Label)
df_num <- cbind(df_num,Label)
df_DA <- df_num[-c(5)]
df_DA$Label <- as.factor(df_DA$Label)
pairs.panels(df_DA[, -5],gap=0, bg=c("red", "green")[df_DA$Label],
pch=21)
```

## LDA Histogram

Split the dataset into two subsets. $70\%$ as training, and $30\%$ as test.\
This plot shows some overlap observed between two groups. and also some of the observations can be observed clearly which group they belong to.\

```{r ldahist, echo=FALSE}
set.seed(2022394)
ind <- sample(c("Train","Test"),
              nrow(df_DA),replace = TRUE,prob = c(.7, .3))
Train <- df_DA[ind=="Train",]
Test <- df_DA[ind=="Test",]
LDA <- lda(Label~age+absences+G1+G2,data=Train)
Pred <- predict(LDA)
ldahist(data=Pred$x[,1],g=Train$Label)
```

## LDA Partition Plot

```{r ldapart, echo=FALSE}
partimat(Label~., data=Train, method="lda",prec=300)
```

## LDA Confusion Matrix

<div class="columns-2">
Training data

|       | Fail   | Pass   |
|-------|--------|--------|
|Fail   | 80     | 15     |
|Pass   | 18     | 175    |
-   Accuracy: 0.89, Precision: 0.84, Recall: 0.82.\

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

Test data

|       | Fail   | Pass   |
|-------|--------|--------|
|Fail   | 27     | 4      |
|Pass   | 5      | 71     |
-   Accuracy: 0.92, Precision: 0.87, Recall: 0.84.\

## Problems 

-   We tried using a Naive Bayes model to see if it gives us a better model, but the result is not as good as LDA, and we do not show the results here.\

-   There are many other variables in the original dataset. They may have some potential relationship between these variables that effect G3 and may give us a totally different result, or explain any anomalies present within the data. We did not have access to these variables hence we do not know the impact it may have on our analysis.\

- The dataset used is only for students in 2005/2006, it is currently 2022... If we were able to acquire a dataset for 2021/2022 or within the last 5 years it would be of more benefit towards the current generation of high school students as the curriculum has and will change over time. 

## Conclusion

-   LDA gave us nice results, and showed what we expected in the plots and the tables.\

-   Only using G1 and G2 to predict G3 seems a good idea, this may give us a better accuracy, and a lower error rate.\

- Using PCA We can reduce the number of variables to 3 instead of using 4 variables to predict the final grade. The PC123 model worked best.\


# Thank you!
