---
title: 'Report #4'
author: "GROUP 9"
classoption: 12pt
output: bookdown::pdf_document2
bibliography: ../../Reference/bib/References.bib
link-citations: true
biblio_style: "apalike"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(ggcorrplot)
require(ggthemes)
require(matlib)
require(bookdown)
require(xtable)
require(moments)
require(gridExtra)
require(GGally)
require(dplyr)
require(stats)
require(HDtest)
require(pander)
require(fitdistrplus)
require(QuantPsyc)

```

```{r, echo=FALSE}
#df only contains categorical column
student=read.table("../../Data/CSV/student-mat.csv",sep=";",header=TRUE)
student$school <- as.factor(student$school)
student$sex <- as.factor(student$sex)
student$failures <- as.factor(student$failures)
df = data.frame(student$school,student$sex,student$failures)
names(df) = c("School", "Sex", "Failures")

# df_num only contains numeric column
df_num = data.frame(student$age, student$absences, student$G1, student$G2, student$G3)
names(df_num) <- c("age", "absences", "G1","G2","G3")
```

\newpage
# Multivariate Normality Check on All Numerical Variables

Let's start looking at the normality of numerical variables without splitting data. Figure&nbsp;\@ref(fig:qq) shows Quantile-Quantile plot of each numerical variables. For all of the plots, the points are off from the diagonal line, indicating non-normality. 

The table \@ref(tab:mtest) shows the results of Mardia’s Test [@mardia] for multivariate normality where they tested: $$H_0: X_i\ follows\ Normal\ distribution$$ $$H_0: X_i\ does\ not\ follow\ Normal\ distribution$$ $$where, \ i =\{Age, Absences, G1, G2, G3\}$$

p-values were close to zero so, we have very strong evidence against the null hypothesis that the variables follow multivariate normal distribution. The results were expected since we did not split the data by factors. Next chapter will explore multivariate normal distribution of data by sex.

```{r qq, fig.cap="Quantile-Quantile Normality Plot", echo=FALSE, warning=FALSE, fig.height=8}
p1 <- ggplot(df_num, aes(sample=age)) +
  stat_qq() + stat_qq_line() +
  labs(x="Theoritical Quantiles", y="Sample Quantile", title="Age")
p2 <- ggplot(df_num, aes(sample=absences)) +
  stat_qq() + stat_qq_line() +
  labs(x="Theoritical Quantiles", y="Sample Quantile", title="Absence")
p3 <- ggplot(df_num, aes(sample=G1)) +
  stat_qq() + stat_qq_line() +
  labs(x="Theoritical Quantiles", y="Sample Quantile", title="G1")
p4 <- ggplot(df_num, aes(sample=G2)) +
  stat_qq() + stat_qq_line() +
  labs(x="Theoritical Quantiles", y="Sample Quantile", title="G2")
p5 <- ggplot(df_num, aes(sample=G3)) +
  stat_qq() + stat_qq_line() +
  labs(x="Theoritical Quantiles", y="Sample Quantile", title="G3")

grid.arrange(p1, p2, p3, p4, p5, nrow = 3,ncol = 2)
```

```{r mtest, echo=FALSE}
#xtable(mult.norm(df_num)$mult.test)
```

\begin{table}[ht]
\caption {Maldia's Test}
\centering
\begin{tabular}{rrrr}
  \hline
 & Beta-hat & kappa & p-val \\ 
  \hline
Skewness & 29.74 & 1957.72 & 0.00 \\ 
  Kurtosis & 71.08 & 42.86 & 0.00 \\ 
   \hline
\end{tabular} (\#tab:mtest)
\end{table}

\newpage
# Assumption checks to compare three test scores (G1, G2, and G3) by sex

Figure&nbsp;\@ref(fig:box) shows the box plot of G1, G2 and G3 by sex. It seems that male student tend to score higher than female student. We want to find whether the score differ by sex. Figure&nbsp;\@ref(fig:den) shows the density plot of tests by sex. It seems variances do not differ much by sex but they might not have normal distribution. To conduct the multivariate analysis on means of vectors of students' test score(Hotelling's $T^2$ test), we will check equal variance and normality assumptions.  

```{r box, fig.height=3, fig.cap="Boxplot", echo=FALSE, warning=FALSE, message=FALSE}
df_all <- cbind(df, df_num) 

b1 <- df_all %>%
  ggplot() +
  geom_boxplot(aes(x=Sex, y=G1, col = Sex), notch = TRUE)
b2 <- df_all %>%
  ggplot() +
  geom_boxplot(aes(x=Sex, y=G2, col = Sex), notch = TRUE)
b3 <- df_all %>%
  ggplot() +
  geom_boxplot(aes(x=Sex, y=G3, col = Sex), notch = TRUE)
grid.arrange(b1, b2, b3,ncol=3)
```

```{r den, fig.height=8, fig.cap="Density plot", echo=FALSE, warning=FALSE, message=FALSE}
g1 <- ggplot(df_all, aes(x=G1, col=Sex,
group=Sex, fill=Sex)) +
geom_density(aes(y=..density..), alpha=.7)

g2 <- ggplot(df_all, aes(x=G2, col=Sex,
group=Sex, fill=Sex)) +
geom_density(aes(y=..density..), alpha=.7)

g3 <- ggplot(df_all, aes(x=G3, col=Sex,
group=Sex, fill=Sex)) +
geom_density(aes(y=..density..), alpha=.7)

grid.arrange(g1, g2, g3,ncol=1)
```

\newpage


## Test for multivariate normality

We will conduct the hypotheses test to check multivariate normal distribution by using Mardia's test. The hypotheses are:

$$H_0: X_{i}\ follows\ multivariate\ normal\ distribution$$ $$H_0: X_{i}\ does\ not\ follow\ multivariate\ normal\ distribution$$ $$where, \ i =\{Female, Male\}\ and\ X = (X_{G1}, X_{G2},X_{G3})$$
The table \@ref(tab:mtestf) and \@ref(tab:mtestm) show the result of multivariate normal test. Since p-values are very small we have very strong evidence against the null hypothesis that the variables for each sex do not have multivariate normal distribution. Since the normality assumptions are violated we should not conduct Hotelling's $T^2$ test.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
df_num_f <- df_num[df_all$Sex == "F", -c(1:2)]
#xtable(mult.norm(df_num_f)$mult.test)
```


\begin{table}[ht]
\caption {Maldia's Test for Female data}
\centering
\begin{tabular}{rrrr}
  \hline
 & Beta-hat & kappa & p-val \\ 
  \hline
Skewness & 13.47 & 466.82 & 0.00 \\ 
  Kurtosis & 29.22 & 18.72 & 0.00 \\ 
   \hline
\end{tabular} (\#tab:mtestf)
\end{table}

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
df_num_m <- df_num[df_all$Sex == "M", -c(1:2)]
#xtable(mult.norm(df_num_m)$mult.test)
```

\begin{table}[ht]
\caption {Maldia's Test for Male data}
\centering
\begin{tabular}{rrrr}
  \hline
 & Beta-hat & kappa & p-val \\ 
  \hline
Skewness & 12.22 & 380.89 & 0.00 \\ 
  Kurtosis & 30.69 & 19.58 & 0.00 \\ 
   \hline
\end{tabular} (\#tab:mtestm)
\end{table}

## Test of equality of covariance matrices

We will also test equality of covariance matrices using the test discussed by [@covtest]. The hypotheses are:

$$H_0: \Sigma_{Female} = \Sigma_{Male}$$
$$H_0: \Sigma_{Female} \neq \Sigma_{Male}$$

The table \@ref(tab:covtest) shows the results from 4 tests about covariance matrix. They all returned high p-values indicating that we have very small evidence against the null hypothesis that the covariance matrices are equal. Therefore, we can assume equal covariance matrix.  

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE} 
cov_res <- testCov(subset(df_all, Sex=="F")[,-c(1:5)], subset(df_all, Sex=="M")[,-c(1:5)])
df_cov <- data.frame(c("HD", "CLX", "Scott", "LC"),
rbind(c(cov_res$HD$statistics, cov_res$HD$p.value),
c(cov_res$CLX$statistics, cov_res$CLX$p.value),
c(cov_res$Scott$statistics, cov_res$Scott$p.value),
c(cov_res[[4]]$statistics, cov_res[[4]]$p.value)))
names(df_cov) <- c("Tests", "Test Statistics", "P.values")

#xtable(df_cov)
```


\begin{table}[ht]
\caption {Multivariate covariance matrix test}
\centering
\begin{tabular}{rlrr}
  \hline
 & Tests & Test Statistics & P.values \\ 
  \hline
1 & HD & 1.01 & 0.63 \\ 
  2 & CLX & 1.01 & 0.64 \\ 
  3 & Scott & 0.66 & 0.51 \\ 
  4 & LC & -0.66 & 0.75 \\ 
   \hline
\end{tabular} (\#tab:covtest)
\end{table}


## Significance test on G3 scores by sex (Kruskal-Wallis test)

Since we can not conduct Hotelling's $T^2$ test (because normality assumption was violated!) we will just test the significant difference in G3 scores by sex (we won't do the test for G1 and G2 because we want to control for family wise error). We used Kruskal Wallis test [@kruskal]:

$$H_0: Median\ of\ G3\ test\ scores\ do\ not\ differ\ among\ sex$$
$$H_1: Median\ of\ G3\ test\ scores\ differ\ among\ sex$$
Kruskal-Wallis test is nonparametric test so, it is robust to non-normality. The table 5 shows the result of Kruskal-Wallis test. Since p-value is around 0.05, we conclude that we have some evidence against the null hypothesis that the medians of G3 scores are same among sex. 

```{r kruskal, echo = FALSE}
pander(kruskal.test(G1 ~ Sex, data = df_all))
```


# More multivariate normal test by splitting data by Sex and Failures

Since we could not detect multivariate normal distribution by sex. We will try to split data by Sex and whether the students have failed the other course (there are 4 datasets in total). The Mardia's test was conducted with the hypotheses that: 

$$H_0: X_{i,j}\ follows\ multivariate\ normal\ distribution$$ $$H_0: X_{i,j}\ does\ not\ follow\ multivariate\ normal\ distribution$$ $$where, \ i =\{Female, Male\}\ ,\ j = \{Never\ failed,\ Have\ failed\} $$ $$and\ X = (X_{age}, X_{absence}, X_{G1}, X_{G2},X_{G3})$$

The table 6,7,8 and 9 show the result of Mardia's test for each dataset. P-values are very small for all tests (except the Kurtosis of data of Male students and who have failed the other course). We are concluding that the numerical variables do not follow multivariate normal distribution even though we split data by sex and failures.

```{r, echo=FALSE}
df_all$fail <- ifelse(df_all$Failures != "0", 1, 0)

f1 <- df_num[df_all$Sex == "F" & df_all$fail == 1,]
m1 <- df_num[df_all$Sex == "M" & df_all$fail == 1,]
f0 <- df_num[df_all$Sex == "F" & df_all$fail == 0,]
m0 <- df_num[df_all$Sex == "M" & df_all$fail == 0,]

set.caption("Female and has failed the other course")
pander(mult.norm(f1)$mult.test)
set.caption("Male and has failed the other course")
pander(mult.norm(m1)$mult.test)
set.caption("Female and has NEVER failed the other course")
pander(mult.norm(f0)$mult.test)
set.caption("Male and has NEVER failed the other course")
pander(mult.norm(m0)$mult.test)
```




\newpage

# The Mahalanobis distance
Mahalanobis Distance is a measure of the distance between a point and a distribution. It works very well for multivariate data because it uses covariance between variables to find the distance between two points. And it works well when variables are highly correlated, even if their scales are not the same.

```{r distance, fig.cap="Mahalanobis distances Plot", echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
mu.hat <- colMeans(df_num)
Sigma.hat <- cov(df_num)

dM <- mahalanobis(df_num, center=mu.hat, cov=Sigma.hat)
upper.quantiles <- qchisq(c(.9, .95, .99), df=9)
density.at.quantiles <- dchisq(x=upper.quantiles, df=9)
cut.points <- data.frame(upper.quantiles, density.at.quantiles)
ggplot(data.frame(dM), aes(x=dM)) +
  geom_histogram(aes(y=..density..), bins=nclass.FD(dM),
                 fill="white", col="black") +
  geom_rug() +
  stat_function(fun="dchisq", args = list(df=9),
                col="red", size=2, alpha=.7, xlim=c(0,25)) +
  geom_segment(data=cut.points,
               aes(x=upper.quantiles, xend=upper.quantiles,
                 y=rep(0,3), yend=density.at.quantiles),
             col="blue", size=2) +
xlab("Mahalanobis distances and cut points") +
ylab("Histogram and density")
```



## Multivariate Outlier Detection

Mahalanobis Distance also gives reliable results when outliers are considered as multivariate.[@outliers] To find outliers, the distance between every point and centre in the multi-dimension data is calculated, and outliers are found by considering these distances.

```{r outliers, fig.cap="Scatter plot of G1 and G3 variables", echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

df_num_compare = df_num[c("G1" , "G3")]
df_num_compare.center = colMeans(df_num_compare)
df_num_compare.cov = cov(df_num_compare)

rad = qchisq(p = 0.95, df = ncol(df_num_compare))
rad  = sqrt(rad)
ellipse <- car::ellipse(center = df_num_compare.center, shape = df_num_compare.cov,
                        radius = rad,
                        segments = 150 , draw = FALSE)

library(ggplot2)

ellipse <- as.data.frame(ellipse)
colnames(ellipse) <- colnames(df_num_compare)

figure <- ggplot(df_num_compare , aes(x = G1 , y = G3)) +
       geom_point(size = 2) +
       geom_polygon(data = ellipse , fill = "orange" , color = "orange" , alpha = 0.5)+
       geom_point(aes(df_num_compare.center[1] , df_num_compare.center[2]) , size = 5 , color = "blue") +
       geom_text( aes(label = row.names(df_num_compare)) , hjust = 1 , vjust = -1.5 ,size = 2.5 ) +
       ylab("G3") + xlab("G1")
       
# Run and display plot
figure

```

Figure \@ref(fig:outliers) is the plot of our data and an ellipse from considering center point and covariance matrix. Blue point on the plot shows the center point. Black points are the observations for G1 — G3 variables. As you can see, there are points outside the orange ellipse. It means that these points might be the outliers. If we consider that this ellipse has been drawn over covariance, center and radius, we can say we might have found the same points as the outlier for Mahalanobis Distance.

```{r dM, echo = FALSE}
df_num$dM <- dM
df_num$surprise <- cut(df_num$dM,
                 breaks= c(0, upper.quantiles, Inf),
                 labels=c("Typical", "Somewhat", "Surprising", "Very"))
pander(table(df_num$surprise))
```
Finally, we have identified the outliers as Very in the above table in our multivariate data. They total number of outliers look pretty similar as the points outside of the ellipse in the scatter plot.

\newpage

# Identify the distribution of interested variables
We are motivated by this dataset since we are interested in what features affect overall math grades. We will mainly focus on exploring the distribution of the G3 variable, which is students' final grades. 

```{r distributions, fig.cap="Cullen and Frey graph of G3 variables", echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
descdist(df_num$G3, discrete = FALSE)
```

Looking at the results in Figure \@ref(fig:distributions) and the R output, the data has a negative skewness and a kurtosis not far from 3, the fit of two common left-skewed distributions could be considered, Weibull and gamma distributions.

\newpage

```{r fit, fig.cap="Goodness-of-fit plots for Weibull and gamma distributions fitted to G3 variable", echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
weibull_dist <- fitdist(df_num$G3, "norm", method = "mge")
gamma_dist <- fitdist(df_num$G3, "gamma", method = "mge")

par(mfrow = c(2, 2))
plot.legend <- c("Weibull", "gamma")
denscomp(list(weibull_dist, gamma_dist), legendtext = plot.legend)
qqcomp(list(weibull_dist, gamma_dist), legendtext = plot.legend)
cdfcomp(list(weibull_dist, gamma_dist), legendtext = plot.legend)
ppcomp(list(weibull_dist, gamma_dist), legendtext = plot.legend)
```

In Figure \@ref(fig:fit), none of the fitted distributions correctly describes the center of the distribution, but the Weibull distribution could be prefered for their better description of the right tail of the empirical distribution, especially if this tail is important in the use of the fitted distribution, as it is in the context of high grades in Maths.[@distributions]

\newpage

# References 