---
title: 'Student Performance Analysis Report'
author: "GROUP 9"
classoption: 12pt
output: bookdown::pdf_document2
bibliography: ../../Reference/bib/References.bib
link-citations: true
biblio_style: "apalike"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(ggplot2)
require(ggthemes)
require(gridExtra)
require(pander)
require(xtable)
require(bookdown)
require(psych)
require(MASS)
require(klaR)
require(HDtest)
require(ggcorrplot)
```


```{r, echo=FALSE}
#df only contains categorical column
student=read.table("../../Data/CSV/student-mat.csv",sep=";",header=TRUE)
student$school <- as.factor(student$school)
student$sex <- as.factor(student$sex)
student$failures <- as.factor(student$failures)
df = data.frame(student$school,student$sex,student$failures)
names(df) = c("School", "Sex", "Failures")

# df_num only contains numeric column
df_num = data.frame(student$age, student$absences, student$G1, student$G2, student$G3)
names(df_num) <- c("age", "absences", "G1","G2","G3")
```

# Author

![](../../Images/JPG/Photo/Fugo%20Takefusa.jpg){width="100px"} 
Fugo Takefusa\
[takefufugo\@myvuw.ac.nz](mailto:takefufugo@myvuw.ac.nz){.email}\
ORCID: 0000-0001-7373-8389\

![](../../Images/JPG/Photo/Rhys%20Lewis-Woodley.jpg){width="100px"} 
Rhys Lewis-Woodley\
[lewisrhys\@myvuw.ac.nz](mailto:lewisrhys@myvuw.ac.nz){.email}\
ORCID: 0000-0001-5414-0832\

![](../../Images/JPG/Photo/Soumya%20Banerjee.jpg){width="100px"} 
Soumya Banerjee\
[banerjsoum\@myvuw.ac.nz](mailto:banerjsoum@myvuw.ac.nz){.email}\
ORCID: 0000-0001-5678-5095\

![](../../Images/JPG/Photo/Vivian%20Dong.jpg){width="100px"} 
Vivian Dong\
[dongting\@myvuw.ac.nz](mailto:dongting@myvuw.ac.nz){.email}\
ORCID: 0000-0001-8356-8598\

![](../../Images/JPG/Photo/Yi%20Zhang.jpg){width="100px"}
Yi Zhang\
[zhangyi11\@myvuw.ac.nz](mailto:zhangyi11@myvuw.ac.nz){.email}\
ORCID: 0000-0003-2959-5290\

# Introduction

The data is about student academic grades for math course of two Portuguese secondary public school during the 2005/2006 school year [@Dataset]. Student grades and their economic, social and demographic variables are included in the data. The data is based on the reports of two public schools (i.e., the grades of students and the number of absence days) and the questionnaire answers by the students. All of the demographic, economic and social variables were collected using the questionnaire. The dataset has 395 records in total with 5 numerical variables and 28 categorical variables. The numerical variables include age, number of school absences, 1st Period grade, 2nd period grade and final period grade. The categorical variables include sex, address(urban/rural), parents highest qualification, family size, commuting time, study time and so on.

We have two main leading questions for our analysis:\

- Can Principal Component Analysis - PCA reduce the number of dimensions to model the students' final grade?\

- Can we have a discriminant model to classify students whether they pass or fail based on the other features(e.g., first test grade, age, etc)?\

Since we have too many categorical variables, for our data analysis, we will only include three categorical variables, which is school, sex and number of past failures.\

# Exploratory Data Analysis - EDA

## Categorical Summary

Three categorical variables:\
School - student's school ('GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\
Sex - student's sex ('F' - female or 'M' - male)\
Failures - number of past class failures (n if 1<=n<3, else 4)\

The table \@ref(tab:summary) and Figure&nbsp;\@ref(fig:ct) show the count of each categorical variables. There are more students from Gabriel Pereira school than Mousinho da Silveira school. There are slightly higher number of female students compared to male students. Most of the students never failed the other courses before.

```{r CateSum, echo=FALSE}
student$school <- as.factor(student$school)
student$sex <- as.factor(student$sex)
student$failures <- as.factor(student$failures)
df = data.frame(student$school,student$sex,student$failures)
names(df) = c("School", "Sex", "Failures")
#xtable(summary(df))
```

\begin{table}[ht]
\centering
\caption {Summary of Three Categorical Variables}
\begin{tabular}{lll}
  \hline
 School & Sex & Failures \\ 
  \hline
GP:349   & F:208   & 0:312   \\ 
MS: 46   & M:187   & 1: 50   \\ 
  &  & 2: 17   \\ 
  &  & 3: 16   \\ 
   \hline
\end{tabular} (\#tab:summary)
\end{table}


```{r ct, fig.height=4, fig.cap="Barplot for Categorical Variables", fig.pos="center", echo=FALSE, warning=FALSE}
lapply(names(df), function(col) {
  ggplot(df, aes(.data[[col]], ..count..)) + 
    geom_bar(aes(fill = .data[[col]]), position = "dodge")
}) -> lp
grid.arrange(lp[[1]], lp[[2]], lp[[3]], ncol = 3)
```

## Numerical Summary
Five numerical Variables:\
Age - student's age (from 15 to 22)\
Absences - number of school absences (from 0 to 93)\
G1 - first period grade (from 0 to 20)\
G2 - second period grade (from 0 to 20)\
G3 - final grade (from 0 to 20, output target)\

The table \@ref(tab:numsum) shows five numeric variable statistic summary includes: minimum, first quartile, median, third quartile, and maximum.\
The table \@ref(tab:means) and table \@ref(tab:covariance) show five numeric variable means and covariance.\
The covariance between age and absences, G1 and G2 and G3, G3 and absences are positive, this means both variables will tend to move upward or downward in value at the same time. For example, when a student have a higher mark in G1, and this student may have higher mark in G3.\
The covariance between age and G1 and G2 and G3, absences and G1 and G2 and G3 are negative, this means variables will move away from each other. For example, a student have more number of absences, this student may get lower mark in G1, G2 and G3.\

```{r NumSum, echo=FALSE}
df_num = data.frame(student$age, student$absences, 
                    student$G1, student$G2, student$G3)
names(df_num) <- c("age", "absences", "G1","G2","G3")
#summary(df_num)
```

\begin{table}[ht]
\centering
\caption {Summary of Five numerical Variables}
\begin{tabular}{llllll}
  \hline
 & age & absences & G1 & G2 & G3 \\ 
  \hline
Min. & 15.0 & 0.0   & 3.0 & 0.0 &  0.0  \\
1st Qu. & 16.0 & 0.0   & 8.0 & 9.0 &  8.0  \\ 
Median & 17.0 & 4.0   & 11.0 & 11.0 &  11.0  \\
3rd Qu. & 18.0 & 8.0   & 13.0 & 13.0 &  14.0  \\
Max. & 22.0 & 75.0   & 19.0 & 19.0 &  20.0  \\
   \hline
\end{tabular} (\#tab:numsum)
\end{table}

```{r nummean,echo=FALSE}
df_num.mean<-colMeans(df_num)
```

\begin{table}[ht]
\centering
\caption {Means of Five numerical Variables}
\begin{tabular}{lllll}
  \hline
 age & absences & G1 & G2 & G3 \\ 
  \hline
16.696   & 5.709   & 10.909 & 10.714 &  10.415  \\ 
   \hline
\end{tabular} (\#tab:means)
\end{table}

```{r covmat,echo=FALSE}
#cov(df_num)
```

\begin{table}[ht]
\centering
\caption {Covariance of Five numerical Variables}
\begin{tabular}{llllll}
  \hline
 & age & absences & G1 & G2 & G3 \\ 
  \hline
age & 1.628   & 1.790   & -0.271 & -0.689 &  -0.945  \\
absences & 1.790   & 64.050   & -0.824 & -0.957 &  1.256  \\ 
G1 & -0.271   & -0.824   & 11.017 & 10.639 &  12.188  \\
G2 & -0.689   & -0.957   & 10.639 & 14.149 &  15.594  \\
G3 & -0.945   & 1.256   & 12.188 & 15.594 &  20.990  \\
   \hline
\end{tabular} (\#tab:covariance)
\end{table}

\newpage
Figure \@ref(fig:numplt) shows the count of five numerical variables. Red dashed line as mean and a fitted density in purple.

```{r numplt,fig.height=4, fig.cap="Barplot for Numerical Variables", fig.pos="center", echo=FALSE, warning=FALSE}
lp2 <- list()

lp2[[1]] <- ggplot(data.frame(df_num$age), aes(x=df_num$age)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$age),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[1], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("Age") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[2]] <- ggplot(data.frame(df_num$absences), aes(x=df_num$absences)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$absences),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[2], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("Absences") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[3]] <- ggplot(data.frame(df_num$G1), aes(x=df_num$G1)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G1),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[3], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G1") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[4]] <- ggplot(data.frame(df_num$G2), aes(x=df_num$G3)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G3),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[4], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G2") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

lp2[[5]] <- ggplot(data.frame(df_num$G3), aes(x=df_num$G3)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(df_num$G3),
                 col="black", fill="white") +
  geom_vline(xintercept = df_num.mean[5], col="red",linetype="dashed")+
  geom_density(kernel="epanechnikov", size=.3, col="purple", alpha=.1) +
  xlab("G3") +
  ylab("Count") +
  theme(text=element_text(size=12, 
        family="serif"))

grid.arrange(lp2[[1]], lp2[[2]], lp2[[3]],lp2[[4]],lp2[[5]],nrow = 2,ncol = 3)
```


## Correlation Matrix and plot

Table \@ref(tab:correlation) and Figure \@ref(fig:corplt) shows the correlation between all five numerical variables.\
From the table and figure we get that G1, G2 and G3 are have a strong positive correlation.\
Between G2 and G3 have the strongest positive correlation, which is 0.905.\
Between age and G1, G2, G3, they have a low negative correlation.\

```{r cormat,echo=FALSE}
#cor(df_num)
```

\begin{table}[ht]
\centering
\caption {Correlation Matrix of Five numerical Variables}
\begin{tabular}{llllll}
  \hline
 & age & absences & G1 & G2 & G3 \\ 
  \hline
age & 1.000   & 0.175   & -0.064 & -0.143 &  -0.162  \\
absences & 0.175   & 1.000   & -0.031 & -0.032 &  0.034  \\ 
G1 & -0.064   & -0.031   & 1.000 & 0.852 &  0.801  \\
G2 & -0.143   & -0.032   & 0.852 & 1.000 &  0.905  \\
G3 & -0.162   & 0.034   & 0.801 & 0.905 &  1.000  \\
   \hline
\end{tabular} (\#tab:correlation)
\end{table}

```{r corplt,fig.height=4, fig.cap="Correlation between five Numerical Variables", fig.pos="center", echo=FALSE, warning=FALSE}
ggcorrplot(cor(df_num),
method = "circle",
hc.order = TRUE,
type = "lower")
```

\newpage
# Methodology

We have used two methods to analyse the student performance dataset. Firstly, we have used Principle Component Analysis to see if we can reduce the number of variables to model the students' final grade. We have four numerical predictors(First test score, Second test score, Number of absences and Age). we would like to reduce the by using PCA. Secondly, we have used Linear Discriminant Analysis to classify the students whether they pass or fail the math course based on the other predictors. We have used 70% of the data to train LDA and 30% of the data to validate the model.


# Principal Component Analysis - PCA

## PCA Details

We have conducted PCA on 4 numeric variables (G1, G2, Age, Number of Absences) to see if we can perform dimentionality reduction. 
The table \@ref(tab:sumPCA) shows summary of PCA results with proportion of variances explained by each component and the cumulative proportion. The first and the second principle components explained 96% of total variance. Figure&nbsp;\@ref(fig:scree) shows the visualization of proportion of variance explained by each principle components. It seems two principle components are enough to explain most of the variance.


```{r scree, fig.cap="Scree Plot of PCA in %", echo=FALSE, warning=FALSE, fig.height=6}
PCA <- prcomp(df_num[, -5])
var_explained = PCA$sdev^2 / sum(PCA$sdev^2)

#create scree plot
qplot(c(1:4), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Proportion of Variance Explained") +
  ggtitle("Scree Plot for PCA") +
  ylim(0, 1)
```


\begin{table}[ht]
\caption {Summary of PCA}
\centering
\begin{tabular}{rrrrr}
  \hline
 & PC1 & PC2 & PC3 & PC4 \\ 
  \hline
Standard deviation & 8.0088 & 4.8287 & 1.4047 & 1.1888 \\ 
  Proportion of Variance & 0.7061 & 0.2567 & 0.0217 & 0.0156 \\ 
  Cumulative Proportion & 0.7061 & 0.9627 & 0.9844 & 1.0000 \\ 
   \hline
\end{tabular} (\#tab:sumPCA)
\end{table}

\newpage

## Fit Linear Regression

We fitted three Linear Regression models. A model which has all numerical predictors in the model, a model that contains the first and second principal components and a last model includes the first, second and third principal components. The model equations are:  

$$
Full\ model:G3_i = \beta_0 + \beta_1G1_i + \beta_2G2_i + \beta_3Age_i + \beta_4Absence_i + \epsilon_i
$$

$$
PC12\ model:G3_i = \beta_0 + \beta_1PC1_i + \beta_2PC2_i + \epsilon_i
$$

$$
PC123\ model:G3_i = \beta_0 + \beta_1PC1_i + \beta_2PC2_i + \beta_3PC3_i + \epsilon_i
$$


Figure&nbsp;\@ref(fig:diag1), &nbsp;\@ref(fig:diag2) and &nbsp;\@ref(fig:diag3) show the diagnostic plots of each model. It seems all models show violation of Normality and errors have some patterns. PCA could not fix the violation assumptions.

```{r diag1, fig.cap="Diagnostic Plot for Full Model", echo=FALSE, warning=FALSE, fig.height=6}
PCAdf <- data.frame(df_num$G3, PCA$x)
modPCA12 <- lm(df_num.G3 ~ PC1 + PC2, data=PCAdf)
modPCA123 <- lm(df_num.G3 ~ PC1 + PC2 + PC3, data=PCAdf)
modNormal <- lm(G3 ~ ., data=df_num)

par(mfrow=c(2,2))
plot(modNormal)
```

```{r diag2, fig.cap="Diagnostic Plot for Model with PC1 and PC2", echo=FALSE, warning=FALSE, fig.height=6}
par(mfrow=c(2,2))
plot(modPCA12)
```

```{r diag3, fig.cap="Diagnostic Plot for Model with PC1, PC2 and PC3", echo=FALSE, warning=FALSE, fig.height=6}
par(mfrow=c(2,2))
plot(modPCA123)
```

\newpage

## Check AIC, BIC and Adjusted $R^2$

We have checked the model fit by AIC, BIC and Adjusted $R^2$. The table  \@ref(tab:IC) shows the results. In contrast with what we have seen in Figure&nbsp;\@ref(fig:scree) scree plot before, the model with the first and second principle components have the highest AIC, BIC and lowest Adjusted $R^2$ value. AIC from full model and AIC from the model with three principle components do not differ much. Model with three principle components have the lowest BIC. Thus, PC123 model is the 'best' model so far. We can conclude that by using PCA, we can reduce the number of variables to 3 instead of using 4 variables to predict G3.

```{r, echo=FALSE, include=FALSE}

tab <- data.frame(AIC(modNormal, modPCA12,modPCA123)[,2],
BIC(modNormal, modPCA12,modPCA123)[,2],
c(summary(modNormal)$adj,
  summary(modPCA12)$adj,
  summary(modPCA123)$adj))
rownames(tab) <- c("Full Model", "ModelPC12", "ModelPC123")
colnames(tab) <- c("AIC", "BIC", "Adjusted R^2")
#xtable(tab)
```

\begin{table}[ht]
\caption {Information Criterion}
\centering
\begin{tabular}{rrrr}
  \hline
 & AIC & BIC & Adjusted $R^2$ \\ 
  \hline
Full Model & 1637.29 & 1661.16 & 0.83 \\ 
  ModelPC12 & 1689.13 & 1705.05 & 0.80 \\ 
  ModelPC123 & 1637.31 & 1657.20 & 0.83 \\ 
   \hline
\end{tabular}  (\#tab:IC)
\end{table}

\newpage
# Linear Discriminant Analysis - LDA

After EDA and PCA, we are familiar with our dataset, and we want to actually use this dataset to predict whether a student will Fail or Pass the math course, if we have all the information except the final grade G3 of this student.\
From our previous analyse in EDA, there might have a way to define G3 by a linear combination of the input variables. Therefore, LDA will be applied.\

## Define Label

Since G3 will be the class label, and the full mark of G3 is $20$, therefore, grades that below $10$ will be labelled as Fail, and equal and above $10$ will be Pass.\
The table shows the total students in each labels, there are $265$ students pass the course, and $130$ students who failed.\

```{r, echo=FALSE}
G3 <- df_num$G3
Label = c()
for(i in 1:length(G3)){
  if(G3[i]>=10){
    Label[i] <- "Pass"
  }else {
    Label[i] <- "Fail"
  }
}
Label <- data.frame(Label)
df_num <- cbind(df_num,Label)
df_DA <- df_num[-c(5)]
df_DA$Label <- as.factor(df_DA$Label)
pander(table(df_DA$Label))
```

## LDA Underlying hypotheses

There are two main hypotheses that need to check before any further analyse.\
- The classes are linearly separable.\
- The covariance matrices are not too different.\

### Pairs plot

Figure \@ref(fig:ldaPairs) shows the pairs plot of 4 numerical varialbes with two class labels. The red dots are Fail, and the green dots are Pass.\
From this plot, we have correlation coefficients, in scatterplots, we can see G1 and G2 are good to separate between two groups, and the two classes are kind of linearly separable, in other cases, there is an overlapping and not a clear separation between two groups.\


```{r ldaPairs, fig.cap="Pairs plot of 4 numerical variables with two class label colors", echo=FALSE}
pairs.panels(df_DA[, -5],gap=0, bg=c("red", "green")[df_DA$Label],
pch=21)
```

\newpage
### Covariance matrix

Table \@ref(tab:covfail) and table \@ref(tab:covpass) are the covariance matrices of two groups.\
We have conducted two-sample HD test to check equal covariance matrices. Table \@ref(tab:covtest) show the result of the test indicating unequal covariance matrices.\ From the equal covariance matrices test result, it shows that LDA might not work well. However, by looking at the covariance matrices, it seems they are practically not much different with each other. Thus, we will continue with LDA.\

```{r, echo=FALSE, message=FALSE, error=FALSE}
df_split<- split(df_DA,df_DA$Label)
covtest <- testCov(df_split$Fail[,-5], df_split$Pass[,-5])

#pander(covtest$HD,caption = "Two-Sample HD test Result")
#cov(df_split$Fail[,-5])
#cov(df_split$Pass[,-5])
```


\begin{table}[ht]
\centering
\caption {Covariance Matrix of Fail Group}
\begin{tabular}{lllll}
  \hline
 & age & absences & G1 & G2 \\ 
  \hline
age      & 1.91  & 1.61   & -0.14 & 0.35 \\
absences & 1.61  & 109.76 & 2.41  & 6.41 \\ 
G1       & -0.14 & 2.41   & 2.90  & 1.70 \\
G2       & 0.35  & 6.41   & 1.70  & 7.36 \\
   \hline
\end{tabular}(\#tab:covfail)
\end{table}

\begin{table}[ht]
\centering
\caption {Covariance Matrix of Pass Group}
\begin{tabular}{lllll}
  \hline
 & age & absences & G1 & G2 \\ 
  \hline
age      & 1.42  & 1.63   & 0.42 & -0.27 \\
absences & 1.63  & 41.14  & 0.03 & -1.55 \\ 
G1       & 0.412 & 0.03   & 7.76 & 6.06 \\
G2       & -0.27 & -1.55  & 6.06 & 6.41 \\
   \hline
\end{tabular}(\#tab:covpass)
\end{table}

\begin{table}[ht]
\centering
\caption {Two-Sample HD test Result}
\begin{tabular}{lll}
  \hline
Test statistics & P value & Alternative hypothesis \\ 
  \hline
 7.519     &   0 * * *     &    two.sided  \\
   \hline
\end{tabular}(\#tab:covtest)
\end{table}

\newpage


## LDA Histogram

We split the dataset into training data and test data randomly, we use $70\%$ of the original dataset as training data, and $30\%$ as test data.\
We first want to train LDA with the training dataset.\
Table \@ref(tab:ldaprob) shows there are $34.0\%$ students in training data are Fail, and $66.0\%$ are Pass. This result is kind of what we expected, since we have $130$ Fail and $265$ Pass.\

```{r include=FALSE}
set.seed(2022394)
ind <- sample(c("Train","Test"),
              nrow(df_DA),replace = TRUE,prob = c(.7, .3))
Train <- df_DA[ind=="Train",]
Test <- df_DA[ind=="Test",]
LDA <- lda(Label~age+absences+G1+G2,data=Train)
```

\begin{table}[ht]
\centering
\caption {Prior probabilities of groups}
\begin{tabular}{ll}
  \hline
Fail & Pass \\ 
  \hline
0.340 & 0.659 \\
   \hline
\end{tabular}(\#tab:ldaprob)
\end{table}

\newpage
Figure \@ref(fig:ldahist) shows that the separation between two groups, but there is an overlapping which is not great. It seems for LDA, it is hard to separate students into two groups for students who have a G3 mark around 10.

```{r ldahist, fig.cap="LDA histogram of two groups",echo=FALSE}
Pred <- predict(LDA)
ldahist(data=Pred$x[,1],g=Train$Label)
```

\newpage
## Partition Plot

Figure \@ref(fig:ldapart) is the partition plot. In general, we have a really nice result with small error rate.\
The smallest error rate is $0.108$ which is the plot of G1 and G2, this result is what we expected, since G1 and G2 have the highest correlation with G3. In the pairs plot above, this also shows that this plot may give us the best result.\

```{r ldapart, fig.cap="Partition plot of training data", echo=FALSE}
partimat(Label~., data=Train, method="lda",prec=300)
```

\newpage
## Confusion matrix

Table \@ref(tab:trainmat) and table \@ref(tab:testmat) are the confusion matrices. From the the matrices, training data has an accuracy of $0.89$ and test data has accuracy of $0.92$, both give us a nice result.\
However, the total students in two groups are not equal, therefore, we need to check Recall and Precision as well.\
In training data, we have precision $0.84$ and recall $0.82$, and in test data we have precision $0.87$ and recall $0.84$. The result here in both training data and test data gives a good result.
In general, LDA gives a quite nice model, and the dataset can fit LDA well.\


```{r, include=FALSE}
optPred <- predict(LDA, Train)$class
confusion_Train <- table(optPred, Actual=Train$Label)
realPred <- predict(LDA,Test)$class
confusion_Test <- table(realPred, Actual=Test$Label)
sum(diag(confusion_Train))/sum(confusion_Train)
sum(diag(confusion_Test))/sum(confusion_Test)
confusion_Train[1]/(confusion_Train[1]+confusion_Train[3])
confusion_Train[1]/(confusion_Train[1]+confusion_Train[2])
confusion_Test[1]/(confusion_Test[1]+confusion_Test[3])
confusion_Test[1]/(confusion_Test[1]+confusion_Test[2])
```

\begin{table}[ht]
\centering
\caption {Confusion matrix of training data}
\begin{tabular}{lll}
  \hline
& Fail & Pass \\ 
  \hline
Fail & 80 & 15 \\
Pass & 18 & 175 \\
   \hline
\end{tabular}(\#tab:trainmat)
\end{table}

\begin{table}[ht]
\centering
\caption {Confusion matrix of test data}
\begin{tabular}{lll}
  \hline
& Fail & Pass \\ 
  \hline
Fail & 27 & 4 \\
Pass & 5 & 71 \\
   \hline
\end{tabular}(\#tab:testmat)
\end{table}

#Discussion

## Problems
-   We tried using a Naive Bayes model to see if it gives us a better model, but the result is not as good as LDA, and we do not show the results here.\

-   There are many other variables in the original dataset. They may have some potential relationship between these variables that effect G3 and may give us a totally different result, or explain any anomalies present within the data. We did not have access to these variables hence we do not know the impact it may have on our analysis.\

- The dataset used is only for students in 2005/2006, whereas it is currently 2022. A dataset for 2021/2022 or within the last 5 years it would be of more benefit towards the current generation of high school students as the curriculum has and will change over time. It would of even better benefit if we could acquire data consisting of results from schools in other countries to provide context as to how other countries compare against each other.\

## Conclusion
-   We believe our analysis of student academic grades for math course of two Portuguese secondary public school collected in 2006 was quite successful. As our two main analysis techniques of PCA and LDA gave us results that aligned with our reasons for using them.\

-   With the PCA we successfully reduced the number of variables to 3 instead of 4. Shown by the favorable AIC, BIC and adjusted R squared scores of the PC123 model.\

-   After verifying assumptions our LDA gave us nice results as well. The G1 and G2 variables (first and second period scores) seemed to be the best indicator for classifying G3. This was represented nicely in the partition plots and later verified in the confusion matrix. This was a very satisfying insight as it aligns with common sense that the performances in other periods would best classify G1 performance.\

-   Although very satisfied in our analysis of the data. It was only in reviewing the collection of this data that we began to question the relevance of this analysis. As mentioned in our problems it is difficult to see how data from 2005/2006 from Portugal holds relevance in 2021/2022. A more modern collection of data would be great next step to analyse especially after the fantastic insights we gained here.\

# References




